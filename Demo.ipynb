{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c63127fb-6496-4ce5-ab89-5f0183bea676",
   "metadata": {},
   "source": [
    "# Demo for SFLKit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b55e972-81c4-4fc0-9da4-416f1c93723f",
   "metadata": {},
   "source": [
    "## What is SFLKit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa52fe9e-abd2-4fc1-9c61-8d05f3c52521",
   "metadata": {},
   "source": [
    "SFLKit is a workbench for statistical fault localization. It comes with the fundamental concepts of statistical debugging and spectrum-based fault localization.\n",
    "\n",
    "You can use SFLKit out-of-the-box by integrating its command-line interface `sfl.py` or as a library, as we do in this demonstration. We designed SFLKit to be highly configurable and expandable with novel concepts.\n",
    "\n",
    "To install SFLKit execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b48fd713-7f8f-4c00-aba6-a8052aa5ed36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/marius/Desktop/work/projects/sflkit\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: astor in /Users/marius/.pyenv/versions/3.9.7/lib/python3.9/site-packages (from sflkit==0.0.1) (0.8.1)\n",
      "Requirement already satisfied: numpy in /Users/marius/.pyenv/versions/3.9.7/lib/python3.9/site-packages (from sflkit==0.0.1) (1.21.2)\n",
      "Building wheels for collected packages: sflkit\n",
      "  Building wheel for sflkit (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sflkit: filename=sflkit-0.0.1-py3-none-any.whl size=36084 sha256=3905ae28e454f777a27417a62c8136a88f497b0b5256d0c9f97c3857558fc6f4\n",
      "  Stored in directory: /private/var/folders/09/pt1hglws43n7fh5521n6zyyh0000gn/T/pip-ephem-wheel-cache-lhtf00j_/wheels/66/6f/12/7a57ca5f6f40197ae4f96b58c9e8b176ee261d1660520b4fbd\n",
      "Successfully built sflkit\n",
      "Installing collected packages: sflkit\n",
      "  Attempting uninstall: sflkit\n",
      "    Found existing installation: sflkit 0.0.1\n",
      "    Uninstalling sflkit-0.0.1:\n",
      "      Successfully uninstalled sflkit-0.0.1\n",
      "Successfully installed sflkit-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8333c71-28ed-44ba-8e4e-52a72f8f96e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "import importlib\n",
    "import inspect\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "from sflkit.color import ColorCode\n",
    "from sflkit import instrument_config, analyze_config\n",
    "from sflkit.config import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be07d73-70af-4d38-9bc9-2d2f027987f0",
   "metadata": {},
   "source": [
    "## A faulty Program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742bbc76-5fd4-46a9-ac78-f132118ec072",
   "metadata": {},
   "source": [
    "First, we need a faulty program. We chose an implementation of the `middle(x, y, z)` function that returns the *middle* number of its three arguments. For example, `middle(1, 3, 2)` should return 2 because `1 < 2` and `2 < 3`. We introduced a fault in this implementation of `middle` that occurs in line 7 `m = y`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97ceacc2-3d6f-4d8f-95b4-11d45e805760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def middle(x, y, z):\n",
    "    m = z\n",
    "    if y < z:\n",
    "        if x < y:\n",
    "            m = y\n",
    "        elif x < z:\n",
    "            m = y  # bug\n",
    "    else:\n",
    "        if x > y:\n",
    "            m = y\n",
    "        elif x > z:\n",
    "            m = x\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3669e4eb-0643-488e-9859-9290dc88eea5",
   "metadata": {},
   "source": [
    "Next, we introduce a class to capture test runs' results efficiently. The `TestResult` is an enum with two possible values, `PASS`and `FAIL`. `PASS` donates a passing test case and `FAIL` a failing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "374e93f7-e48b-46fd-997d-54dea2b67e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestResult(enum.Enum):\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.value\n",
    "    \n",
    "    PASS = 'PASS'\n",
    "    FAIL = 'FAIL'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89df6ffe-a6b3-4a56-bd07-cd74a0fb438e",
   "metadata": {},
   "source": [
    "Now we implement a test function that takes the three arguments of `middle(x, y, z)` and an expected result. This test function compares the return of `middle(x, y, z)` with the desired value and returns `PASS` if they match and `FAIL` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eea55fff-0efa-48ba-8112-ed6f5698b645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_middle(x, y, z, expected):\n",
    "    try:\n",
    "        if middle(x, y, z) == expected:\n",
    "            return TestResult.PASS\n",
    "        else:\n",
    "            return TestResult.FAIL\n",
    "    except BaseException:\n",
    "        return TestResult.FAIL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe80e8c-0bac-4f7e-9919-51b2befb7452",
   "metadata": {},
   "source": [
    "Let's check the results for some combinations of the numbers 1, 2, and 3. The expected value is in all cases 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "549d8951-4d04-4d38-baf6-f561e5d72888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PASS"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_middle(3, 2, 1, expected=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45baa555-b8a8-48d1-bb82-02614bcd7c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PASS"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_middle(3, 1, 2, expected=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b35abe6-1ee9-48cc-a327-2db6c05a1784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FAIL"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_middle(2, 1, 3, expected=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8296a6c8-413e-4d4e-aa6d-ed43c8b51c7d",
   "metadata": {},
   "source": [
    "As you can see, the result of `middle(2, 1, 3)` does not match the expected value 2. Hence, we found a failing test case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf75f32-f4c3-4d96-a258-569da3b882e0",
   "metadata": {},
   "source": [
    "## Instrument the Program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f1e1e5-3b88-4f0c-a72c-b15215c0ad5f",
   "metadata": {},
   "source": [
    "Subsequently, we want to leverage SFLKit to find the location in the code that is most likely to include the fault.\n",
    "\n",
    "Let us first get the source of our function and write it to a file so we have something to perform our instrumentation and analysis.\n",
    "\n",
    "We leverage Python's `inspect` to get the source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d395a695-ba1d-47e9-b21f-ef2d6c95fb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def middle(x, y, z):\n",
      "    m = z\n",
      "    if y < z:\n",
      "        if x < y:\n",
      "            m = y\n",
      "        elif x < z:\n",
      "            m = y  # bug\n",
      "    else:\n",
      "        if x > y:\n",
      "            m = y\n",
      "        elif x > z:\n",
      "            m = x\n",
      "    return m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source = inspect.getsource(middle)\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b9faaf-25ca-4b33-aad6-b7566da54913",
   "metadata": {},
   "source": [
    "We also define the file we write the source to and the python file we will work on, namely `middle.py` and `tmp.py`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76d8e864-1983-4d28-9403-141d20477d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_py = 'middle.py'\n",
    "tmp_py = 'tmp.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3cfffa-93b4-45a6-bfc6-2906da72d538",
   "metadata": {},
   "source": [
    "We write the source code to `middle.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cac1b2a-a3f4-45ca-a852-897dc1b754d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(middle_py, 'w') as fp:\n",
    "    fp.write(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d417ea19-6938-44b3-999c-06d625c10206",
   "metadata": {},
   "source": [
    "Let's update our test function to import the correct module and run the `middle(x, y, z)` from this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "085526f9-fadb-4b73-a5f6-df6e6f82a267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_middle_import(x, y, z, expected):\n",
    "    from middle import middle\n",
    "    try:\n",
    "        if middle(x, y, z) == expected:\n",
    "            return TestResult.PASS\n",
    "        else:\n",
    "            return TestResult.FAIL\n",
    "    except BaseException:\n",
    "        return TestResult.FAIL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88260b4-24b9-4381-a361-a4da931bb1e7",
   "metadata": {},
   "source": [
    "We repeat the tests to check that our setup works with the import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7eb154e9-e4c4-4be6-a8ee-ee1c3f1d1fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PASS, PASS, FAIL)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_middle_import(3, 2, 1, expected=2), test_middle_import(3, 1, 2, expected=2), test_middle_import(2, 1, 3, expected=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c623cfd-eb78-4fa3-afc8-85a82d48002b",
   "metadata": {},
   "source": [
    "We produced the same results for the test cases, so it seems to work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12423b92-7543-4017-9891-6bad2a2994e1",
   "metadata": {},
   "source": [
    "### Configuring SFLKit\n",
    "\n",
    "The `Config` class provides comfortable access to `SFLKit` by defining the fundamental concepts we want to investigate.\n",
    "\n",
    "We give some information for the config that we need to define. First, we need the path to the source we want to investigate, which we already have in `middle_py`. Next, we need an out, `tmp_py`. We also need:\n",
    "\n",
    "The language of our subject is `'python'`.\n",
    "Let's start with `'line'` as the predicates we want to investigate.\n",
    "We define `'tarantula'` as our evaluation metric for the predicates, i.e., the similarity coefficient.\n",
    "We also need a list of passing and failing tests used during the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e43c061a-807b-4bc3-b707-7e1c89799f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "language='python'\n",
    "predicates='line'\n",
    "metrics='Tarantula'\n",
    "passing='event-files/0,event-files/1'\n",
    "failing='event-files/2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc586ae-8d1d-4bb0-987c-cdd608f6ea20",
   "metadata": {},
   "source": [
    "We define a function that gives as a `Config` object, so we do not need to create it manually every time we change something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0cd0637-7b4a-4736-80a3-6211b7e529c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    return Config.config(path=middle_py, working=tmp_py, language=language, predicates=predicates, metrics=metrics, passing=passing, failing=failing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a905e276-c475-4fc4-b23c-936dd5fea818",
   "metadata": {},
   "source": [
    "Now we can define a function that instruments our subject. We leverage `SFLKit`'s `instrument_config()`, which takes a config we create with our defined `get_config()` and instruments the subject. We can also show the content of the instrumented python file with this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0a87958-fbf9-4ad8-91c5-c4f9526ebcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instrument(out=True):\n",
    "    instrument_config(get_config())\n",
    "    if out:\n",
    "        with open(tmp_py, 'r') as fp:\n",
    "            print(fp.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65524f0a-7c30-41db-be30-a108261e5592",
   "metadata": {},
   "source": [
    "Now we instrument our `middle.py` subject and check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c19129e7-cf49-4028-8997-76afeafc9324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sflkit.instrumentation.lib\n",
      "\n",
      "\n",
      "def middle(x, y, z):\n",
      "    sflkit.instrumentation.lib.add_line_event('middle.py', 2, 0)\n",
      "    m = z\n",
      "    sflkit.instrumentation.lib.add_line_event('middle.py', 3, 1)\n",
      "    if y < z:\n",
      "        sflkit.instrumentation.lib.add_line_event('middle.py', 4, 2)\n",
      "        if x < y:\n",
      "            sflkit.instrumentation.lib.add_line_event('middle.py', 5, 3)\n",
      "            m = y\n",
      "        else:\n",
      "            sflkit.instrumentation.lib.add_line_event('middle.py', 6, 4)\n",
      "            if x < z:\n",
      "                sflkit.instrumentation.lib.add_line_event('middle.py', 7, 5)\n",
      "                m = y\n",
      "    else:\n",
      "        sflkit.instrumentation.lib.add_line_event('middle.py', 9, 6)\n",
      "        if x > y:\n",
      "            sflkit.instrumentation.lib.add_line_event('middle.py', 10, 7)\n",
      "            m = y\n",
      "        else:\n",
      "            sflkit.instrumentation.lib.add_line_event('middle.py', 11, 8)\n",
      "            if x > z:\n",
      "                sflkit.instrumentation.lib.add_line_event('middle.py', 12, 9)\n",
      "                m = x\n",
      "    sflkit.instrumentation.lib.add_line_event('middle.py', 13, 10)\n",
      "    return m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "instrument()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9c03c5-419d-49a0-accb-53b190d200dd",
   "metadata": {},
   "source": [
    "As you can see, the instrumentation added an import at the beginning to a lib that comes with `SFLKit`, cluing the execution of files together. Moreover, the instrumentation added a function call function of the lib in front of each executable line that tracks the executed lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690cf88f-0965-4ee7-9ea3-84e0a536e08d",
   "metadata": {},
   "source": [
    "## Get the Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5daf879-9158-402f-8b5f-bab67f4736da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tmp(x, y, z, expected): \n",
    "    import tmp\n",
    "    importlib.reload(tmp)\n",
    "    tmp.sflkit.instrumentation.lib.reset()\n",
    "    try:\n",
    "        if tmp.middle(x, y, z) == expected:\n",
    "            return TestResult.PASS\n",
    "        else:\n",
    "            return TestResult.FAIL\n",
    "    except BaseException:\n",
    "        return TestResult.FAIL\n",
    "    finally:\n",
    "        tmp.sflkit.instrumentation.lib.dump_events()\n",
    "        del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "823303be-b13f-4405-9186-857625aec1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_files = 'event-files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e084ac5b-b15e-47d0-8cda-4dc14adbb2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests():\n",
    "    if os.path.exists(event_files):\n",
    "        shutil.rmtree(event_files)\n",
    "    os.mkdir(event_files)\n",
    "    os.environ['EVENTS_PATH'] = os.path.join(event_files, '0')\n",
    "    test_tmp(3, 2, 1, expected=2)\n",
    "    os.environ['EVENTS_PATH'] = os.path.join(event_files, '1')\n",
    "    test_tmp(3, 1, 2, expected=2)\n",
    "    os.environ['EVENTS_PATH'] = os.path.join(event_files, '2')\n",
    "    test_tmp(2, 1, 3, expected=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46c39b88-bd21-4eba-9461-1138f84e2fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d922df2e-39a9-4aea-a926-b9275bdd9197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze():\n",
    "    run_tests()\n",
    "    return analyze_config(get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f52db0b7-4423-49a2-b79c-3bff82e9f084",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd538827-c2b6-45fe-a3bf-1bae6e213223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LINE': {'Tarantula': [[middle.py:7]:1.0,\n",
       "   [middle.py:6]:0.6666666666666666,\n",
       "   [middle.py:4]:0.6666666666666666,\n",
       "   [middle.py:13]:0.5,\n",
       "   [middle.py:3]:0.5,\n",
       "   [middle.py:2]:0.5,\n",
       "   [middle.py:10]:0.0,\n",
       "   [middle.py:9]:0.0]}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "054f4cb9-becc-4445-aceb-d7790ab40907",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = ColorCode(results['LINE']['Tarantula'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd6bf4bb-9ba9-471b-9355-f27ed8cba390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><pre title=\"Line 1: not executed\">   1      def middle(x, y, z):</pre>\n",
       "<pre style=\"background-color:#ffff99\"\n",
       "                title=\"Line 2:  50%\">   2  50%     m = z</pre>\n",
       "<pre style=\"background-color:#ffff99\"\n",
       "                title=\"Line 3:  50%\">   3  50%     if y &lt; z:</pre>\n",
       "<pre style=\"background-color:#ffdd99\"\n",
       "                title=\"Line 4:  66%\">   4  66%         if x &lt; y:</pre>\n",
       "<pre title=\"Line 5: not executed\">   5                  m = y</pre>\n",
       "<pre style=\"background-color:#ffdd99\"\n",
       "                title=\"Line 6:  66%\">   6  66%         elif x &lt; z:</pre>\n",
       "<pre style=\"background-color:#ff9999\"\n",
       "                title=\"Line 7: 100%\">   7 100%             m = y  # bug</pre>\n",
       "<pre title=\"Line 8: not executed\">   8          else:</pre>\n",
       "<pre style=\"background-color:#b3e6b3\"\n",
       "                title=\"Line 9:   0%\">   9   0%         if x &gt; y:</pre>\n",
       "<pre style=\"background-color:#b3e6b3\"\n",
       "                title=\"Line 10:   0%\">  10   0%             m = y</pre>\n",
       "<pre title=\"Line 11: not executed\">  11              elif x &gt; z:</pre>\n",
       "<pre title=\"Line 12: not executed\">  12                  m = x</pre>\n",
       "<pre style=\"background-color:#ffff99\"\n",
       "                title=\"Line 13:  50%\">  13  50%     return m</pre>\n",
       "<pre title=\"Line 14: not executed\">  14      &nbsp;</pre>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(code.code(middle_py, source, color=True, suspiciousness=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21426bec-6997-490d-8bd8-f836ea28f4f1",
   "metadata": {},
   "source": [
    "## Change the Analysis Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3f66c0-1131-4017-bbd2-a4d68eec4c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicates='def_use'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b53a0bf-49d1-4da2-aadf-3e0eabc7e75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "instrument(out=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265f0d20-560a-4e95-9584-8d5b605e81ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ae9292-5188-41fd-acfa-dcb22721962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = ColorCode(results['DEF_USE']['Tarantula'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76df291a-fa5c-4f57-9d03-fcfbb030de11",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(code.code(middle_py, source, color=True, suspiciousness=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc111ff-ea08-46d5-ad10-4ea87253a9ef",
   "metadata": {},
   "source": [
    "# Change the Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416d8d9f-2636-4e79-8dd4-83d5c3a3e68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics='Ochiai'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7037be-eff7-4da8-a717-6178ce17be1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "instrument(out=False)\n",
    "results = analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e8cbb9-ba19-49a9-b571-b18b409d9d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = ColorCode(results['DEF_USE']['Ochiai'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0132c8-38ec-40d3-8581-7610a0db72a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(code.code(middle_py, source, color=True, suspiciousness=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
